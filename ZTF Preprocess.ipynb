{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Review and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from data import create_record\n",
    "import ztf # shared scritp where the util functions are stored\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transient  = ['SNIa', 'nonSNIa']#['SNIa', 'SNIbc', 'SNII', 'SNIIn', 'SNIIb', 'SLSN', 'TDE']\n",
    "stochastic = ['QSO', 'AGN', 'Blazar', 'CVNova', 'YSO']\n",
    "periodic   = ['LPV', 'RSCVn', 'Ceph', 'EA', 'EBEW', 'RRLc', 'RRLab', 'DSCT', 'Periodic-Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_code = transient+stochastic+periodic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting lightcurves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_meta = './data/raw_data/ztf_forced/metadata.csv'\n",
    "path_dete = './data/raw_data/ztf_forced/detections.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.33 s, sys: 520 ms, total: 8.85 s\n",
      "Wall time: 8.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "detections = pd.read_csv(path_dete)\n",
    "metadata   = pd.read_csv(path_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    6455579\n",
       "1    4826663\n",
       "3     236197\n",
       "Name: fid, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections['fid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detections[detections['fid'] == 1] # single filter only\n",
    "detections = detections[['oid', 'mjd', 'forcediffimflux', 'forcediffimfluxunc', 'forcediffimsnr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train val test 5-fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing forced_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cridonoso/Documents/2PhaseOrNot2Phase/ztf.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(light_curves), np.array(labels), np.array(oids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: (26862, 200, 4)  - (26862,)\n",
      "VALID: (2916, 200, 4)  - (2916,)\n",
      "TEST:  (1593, 992, 4) - (1593,)\n",
      "[INFO] Processing forced_1\n",
      "TRAIN: (26901, 200, 4)  - (26901,)\n",
      "VALID: (2877, 200, 4)  - (2877,)\n",
      "TEST:  (1593, 992, 4) - (1593,)\n",
      "[INFO] Processing forced_2\n",
      "TRAIN: (26857, 200, 4)  - (26857,)\n",
      "VALID: (2921, 200, 4)  - (2921,)\n",
      "TEST:  (1593, 992, 4) - (1593,)\n",
      "[INFO] Processing forced_3\n",
      "TRAIN: (26814, 200, 4)  - (26814,)\n",
      "VALID: (2964, 200, 4)  - (2964,)\n",
      "TEST:  (1593, 992, 4) - (1593,)\n",
      "[INFO] Processing forced_4\n",
      "TRAIN: (26773, 200, 4)  - (26773,)\n",
      "VALID: (3005, 200, 4)  - (3005,)\n",
      "TEST:  (1593, 992, 4) - (1593,)\n"
     ]
    }
   ],
   "source": [
    "name = 'forced'\n",
    "for fold_n in range(5):\n",
    "    dataset_name = '{}_{}'.format(name, fold_n)\n",
    "    print('[INFO] Processing {}'.format(dataset_name))\n",
    "    train_df = metadata[metadata['partition'] == 'training_{}'.format(fold_n)]\n",
    "    val_df   = metadata[metadata['partition'] == 'validation_{}'.format(fold_n)]\n",
    "    test_df  = metadata[metadata['partition'] == 'test']\n",
    "    \n",
    "    result = pd.merge(detections,\n",
    "                  test_df[['oid', 'alerceclass', 'partition']],\n",
    "                  on='oid')\n",
    "    \n",
    "    x_train, y_train, id_train = ztf.get_light_curves(train_df, detections, class_code, n_min=10)\n",
    "    x_val, y_val, id_val = ztf.get_light_curves(val_df, detections, class_code, n_min=10)\n",
    "    x_test, y_test, id_test = ztf.get_light_curves(test_df, detections, class_code, n_min=0)\\\n",
    "    \n",
    "    subsets = {'train':dict(), 'validation':dict(), 'test':dict()}\n",
    "    subsets['train']['x'] = x_train\n",
    "    subsets['train']['y'] = y_train\n",
    "    subsets['train']['oid'] = id_train\n",
    "\n",
    "    subsets['validation']['x'] = x_val\n",
    "    subsets['validation']['y'] = y_val\n",
    "    subsets['validation']['oid'] = id_val\n",
    "\n",
    "    subsets['test']['x'] = x_test\n",
    "    subsets['test']['y'] = y_test\n",
    "    subsets['test']['oid'] = id_test\n",
    "    \n",
    "    X_train, y_train, m_train, o_train = ztf.pad_lightcurves(subsets['train']['x'], \n",
    "                                                             subsets['train']['y'], \n",
    "                                                             subsets['train']['oid'],\n",
    "                                                             maxobs=200)\n",
    "    X_valid, y_valid, m_valid, o_valid = ztf.pad_lightcurves(subsets['validation']['x'], \n",
    "                                                             subsets['validation']['y'], \n",
    "                                                             subsets['validation']['oid'],\n",
    "                                                             maxobs=200)\n",
    "    maxobs_testing = np.max([x.shape[0] for x in subsets['test']['x']])\n",
    "    X_test, y_test, m_test, o_test = ztf.pad_lightcurves(subsets['test']['x'], \n",
    "                                                 subsets['test']['y'],\n",
    "                                                 subsets['test']['oid'],\n",
    "                                                 maxobs=maxobs_testing)\n",
    "\n",
    "    create_record(light_curves=X_train, \n",
    "              labels=y_train, \n",
    "              masks=m_train, \n",
    "              oids=o_train, \n",
    "              path='./data/records/{}/train'.format(dataset_name))\n",
    "    create_record(light_curves=X_valid, \n",
    "                  labels=y_valid, \n",
    "                  masks=m_valid, \n",
    "                  oids=o_valid, \n",
    "                  path='./data/records/{}/val'.format(dataset_name))\n",
    "    create_record(light_curves=X_test, \n",
    "                  labels=y_test, \n",
    "                  masks=m_test, \n",
    "                  oids=o_test, \n",
    "                  path='./data/records/{}/test'.format(dataset_name))\n",
    "\n",
    "    print('TRAIN: {}  - {}\\nVALID: {}  - {}\\nTEST:  {} - {}'.format(X_train.shape, y_train.shape,\n",
    "                                                              X_valid.shape, y_valid.shape,\n",
    "                                                              X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 3, dpi=200, figsize=(10,5),sharey=True, \n",
    "#                         gridspec_kw={'hspace': 0., 'wspace': 0.05},)\n",
    "\n",
    "# for i, (labs, name) in enumerate(zip([y_train, y_valid, y_test], ['train', 'val', 'test'])):\n",
    "    \n",
    "#     uniques, counts = np.unique(labs, return_counts=True)\n",
    "\n",
    "#     LABELS = [class_code[u] for u in uniques]\n",
    "\n",
    "#     axes[i].barh(uniques, counts, align='center', color='darkblue')\n",
    "\n",
    "#     for u, c in zip(uniques, counts):\n",
    "#         axes[i].text(c+1000, u-0.2, '{}'.format(c), fontsize=12)\n",
    "    \n",
    "#     x_ticks = range(len(uniques))\n",
    "#     axes[i].set_yticks(x_ticks)\n",
    "#     axes[i].set_yticklabels(LABELS)\n",
    "\n",
    "#     axes[i].set_title('{}set distribution'.format(name), fontsize=15)\n",
    "#     axes[i].set_xticks([])\n",
    "#     axes[i].set_xlim(0, 30000)\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from data import load_record\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Taking 1 balanced batches\n"
     ]
    }
   ],
   "source": [
    "dataset = load_record('./data/records/forced_0/train', batch_size=32, take=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x, y, m in dataset:    \n",
    "    y_int = np.argmax(y, 1)\n",
    "    unique, counts = np.unique(y_int, return_counts=True)\n",
    "    x_range = range(len(unique))\n",
    "    plt.bar(x_range, counts)\n",
    "    plt.xticks(x_range, unique)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
